{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "328af6f71b4f436f84a27e7a4d5a11e1",
      "7b12ba0ab2f44e719d5921920817279c",
      "76426373bc3a4ffda6c3984a9f0f2a5f",
      "bd0c4e632de44d0f8e6a62a91b9739b4",
      "e17110f63fe94a03977d94b2052e81ce",
      "8ccae21f1b2f49368bbafb3527775d1d",
      "49a2b0898de448d69e2f91f9c93dd7ca",
      "9b4e66387fe44459ad16c28166c71215",
      "865f163380284253b5eb8ab2b4de895d",
      "a2cdaee2a47c40bba7ce98e8f53a4014",
      "a25bdf0d56a44bf4823caf22e83e17b5"
     ]
    },
    "executionInfo": {
     "elapsed": 22217,
     "status": "ok",
     "timestamp": 1652082822222,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "kJXBnhcO1s22",
    "outputId": "0f5caa64-0606-4bb6-d986-e92a37ef23bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "# generator parameters: 734219\n",
      "# discriminator parameters: 5215425\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "import torchvision.models as models\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import glob\n",
    "import skimage.color as sc\n",
    "\n",
    "\n",
    "import os \n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir('/content/gdrive/My Drive/SRGAN')\n",
    "\n",
    "from loss import GeneratorLoss\n",
    "from dataload.srdataload import srDataset\n",
    "from utils.util import psnr , PSNR , predict_simage\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "learning_rate = 6.3e-4  #6.3e-4 1e-5\n",
    "\n",
    "class residualBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(residualBlock,self).__init__()\n",
    "        self.channels = 3\n",
    "        self.num_filter = 64\n",
    "        self.conv1= nn.Conv2d (self.num_filter,self.num_filter,(3,3) ,stride=1,padding = 'same')\n",
    "        self.batchn1 = nn.BatchNorm2d (self.num_filter)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.conv2= nn.Conv2d (self.num_filter,self.num_filter,(3,3) ,stride=1,padding = 'same')\n",
    "        self.batchn2 = nn.BatchNorm2d (self.num_filter)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        in_x = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.batchn1(out)\n",
    "        out = self.prelu1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchn2(out)\n",
    "        out = out + in_x\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class  Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.channels = 3\n",
    "        self.num_filter = 64\n",
    "        self.conv1= nn.Conv2d (self.channels,self.num_filter,(9,9) ,stride=1,padding=4)\n",
    "        self.prelu1 = nn.PReLU()\n",
    "        self.f1 = self.f_layers(residualBlock,5)\n",
    "        \n",
    "        self.conv2= nn.Conv2d (self.num_filter,self.num_filter,(3,3) ,stride=1,padding =1)\n",
    "        self.batchn2 = nn.BatchNorm2d (self.num_filter)\n",
    "        \n",
    "        # 卷积 后放大图片\n",
    "        self.conv3 = nn.Conv2d (self.num_filter,4 * self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.pshuffle1 = nn.PixelShuffle(2)\n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.conv4 = nn.Conv2d ( self.num_filter,4 * self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.pshuffle2 = nn.PixelShuffle(2)\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        \n",
    "        self.conv5= nn.Conv2d (self.num_filter,self.channels,(9,9) ,stride=1,padding=4)\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "    def f_layers ( self,block,num):\n",
    "        layers =[]\n",
    "        for i in range(num):\n",
    "            layers.append(block())\n",
    "            \n",
    "        return nn.Sequential ( *layers )\n",
    "            \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.prelu1(out)\n",
    "        in_out = out\n",
    "        out = self.f1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batchn2(out)\n",
    "        out = in_out + out\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.pshuffle1(out)\n",
    "        out = self.prelu3(out)\n",
    "        \n",
    "        out = self.conv4(out)\n",
    "        out = self.pshuffle2(out)\n",
    "        out = self.prelu4(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        \n",
    "        return (torch.tanh(out) + 1)/2\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.channels = 3\n",
    "        self.num_filter = 64\n",
    "        self.conv1= nn.Conv2d (self.channels,self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.lrelu1 = nn.LeakyReLU(0.2)\n",
    "    \n",
    "        self.conv2= nn.Conv2d (self.num_filter,self.num_filter,(3,3) ,stride=2,padding=1)\n",
    "        self.batchn2 = nn.BatchNorm2d (self.num_filter)\n",
    "        self.lrelu2 = nn.LeakyReLU(0.2)\n",
    "       \n",
    "        self.conv3= nn.Conv2d (self.num_filter,2 * self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.batchn3 = nn.BatchNorm2d ( 2 * self.num_filter)\n",
    "        self.lrelu3 = nn.LeakyReLU(0.2)\n",
    "        self.conv4= nn.Conv2d ( 2 * self.num_filter,2 * self.num_filter,(3,3) ,stride=2,padding=1)\n",
    "        self.batchn4 = nn.BatchNorm2d ( 2 * self.num_filter)\n",
    "        self.lrelu4 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv5= nn.Conv2d (2 * self.num_filter,4 * self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.batchn5 = nn.BatchNorm2d ( 4 * self.num_filter)\n",
    "        self.lrelu5 = nn.LeakyReLU(0.2)\n",
    "        self.conv6= nn.Conv2d ( 4 * self.num_filter,4 * self.num_filter,(3,3) ,stride=2,padding=1)\n",
    "        self.batchn6 = nn.BatchNorm2d ( 4 * self.num_filter)\n",
    "        self.lrelu6 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.conv7= nn.Conv2d (4 * self.num_filter,8 * self.num_filter,(3,3) ,stride=1,padding=1)\n",
    "        self.batchn7 = nn.BatchNorm2d ( 8 * self.num_filter)\n",
    "        self.lrelu7 = nn.LeakyReLU(0.2)\n",
    "        self.conv8= nn.Conv2d ( 8 * self.num_filter,8 * self.num_filter,(3,3) ,stride=2,padding=1)\n",
    "        self.batchn8 = nn.BatchNorm2d ( 8 * self.num_filter)\n",
    "        self.lrelu8 = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "           \n",
    "            \n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        out = self.conv1(x)\n",
    "        out = self.lrelu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.batchn2(out)\n",
    "        out = self.lrelu2(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.batchn3(out)\n",
    "        out = self.lrelu3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.batchn4(out)\n",
    "        out = self.lrelu4(out)\n",
    "        \n",
    "        out = self.conv5(out)\n",
    "        out = self.batchn5(out)\n",
    "        out = self.lrelu5(out)\n",
    "        out = self.conv6(out)\n",
    "        out = self.batchn6(out)\n",
    "        out = self.lrelu6(out)\n",
    "        \n",
    "        out = self.conv7(out)\n",
    "        out = self.batchn7(out)\n",
    "        out = self.lrelu7(out)\n",
    "        out = self.conv8(out)\n",
    "        out = self.batchn8(out)\n",
    "        out = self.lrelu8(out)\n",
    "           \n",
    "        out = self.fc(out).view(batch_size)\n",
    "        \n",
    "        return torch.sigmoid(out)\n",
    "        \n",
    "\n",
    "def train_model( g_model,d_model,dataset1,steps,batch_size ,loss1,device,learning_rate ):\n",
    "    train_dataloader = DataLoader(dataset1, batch_size= batch_size, shuffle=True)\n",
    "    \n",
    "    loss_f = loss1\n",
    "    size = len(train_dataloader.dataset)\n",
    "    \n",
    "    optimizerG = optim.Adam(g_model.parameters(),learning_rate)\n",
    "    optimizerD = optim.Adam(d_model.parameters(),learning_rate)\n",
    "  \n",
    "    img_path = 'test_imgs/b_4down3.jpg'\n",
    "    img_path2 = 'test_imgs/butterfly.png'\n",
    "    #img_path = 'test_imgs/down1.jpg'\n",
    "    #img_path2 = 'test_imgs/t2_1.png'\n",
    "    data_transform = transforms.Compose([\n",
    "    transforms.ToTensor()#,transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "    \n",
    "    for step in range( steps):\n",
    "        g_model.train()\n",
    "        d_model.train()\n",
    "        for batch , (X, y) in enumerate(train_dataloader):\n",
    "        # Compute prediction and loss\n",
    "           \n",
    "            input1 ,target = Variable(X), Variable(y, requires_grad=False)\n",
    "            input1 = input1.to(device)\n",
    "            target = target.to(device)\n",
    "           \n",
    "            real_img =target\n",
    "            fake_img = g_model(input1)\n",
    "             # 更新判别器D的参数\n",
    "            d_model.zero_grad()\n",
    "            real_out = d_model(real_img).mean()\n",
    "            fake_out = d_model(fake_img).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "            \n",
    "            # 更新生成器G的参数\n",
    "            g_model.zero_grad()\n",
    "            \n",
    "            fake_img = g_model(input1)\n",
    "            fake_out = d_model(fake_img).mean()\n",
    "            ##\n",
    "            #print ( fake_out.shape)\n",
    "            #print ( fake_img.shape)\n",
    "            #print ( real_img.shape)\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            g_loss.backward()\n",
    "            \n",
    "            fake_img = g_model(input1)\n",
    "            fake_out = d_model(fake_img).mean()\n",
    "            \n",
    "            \n",
    "            optimizerG.step()\n",
    "       \n",
    "            predict_simage ( g_model,img_path,img_path2,data_transform,device)\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = g_loss.item(), batch * len(X)\n",
    "                predict_simage ( g_model,img_path,img_path2,data_transform,device)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "            ''' if (batch+1) % 500 == 0:\n",
    "              torch.save(g_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'gmodel_weights2.pth')\n",
    "              torch.save(d_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'dmodel_weights2.pth')\n",
    "            if (batch+1) % 700 == 0:\n",
    "              torch.save(g_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'gmodel_weights2.pth')\n",
    "              torch.save(d_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'dmodel_weights2.pth')\n",
    "            \n",
    "            if (batch+1) % 1000 == 0:\n",
    "              torch.save(g_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'gmodel_weights2.pth')\n",
    "              torch.save(d_model.state_dict(), 'save_models/'+str(step)+'_'+str(batch)+'dmodel_weights2.pth')'''\n",
    "\n",
    "\n",
    "        torch.save(g_model.state_dict(), 'save_models_d/'+str(step)+'gmodel_weights2.pth')\n",
    "        torch.save(d_model.state_dict(), 'save_models_d/'+str(step)+'dmodel_weights2.pth')\n",
    "        predict_simage ( g_model,img_path,img_path2,data_transform,device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #hr_path = r'gan_data/hr_hdf5_file.h5'\n",
    "    #lr_path = r'gan_data/lr_hdf5_file.h5'\n",
    "    hr_path = r'D:/xiao mai/SR_datasets/SR/processed/gan_data/hr_hdf5_file.h5'\n",
    "    lr_path =  r'D:/xiao mai/SR_datasets/SR/processed/gan_data/lr_hdf5_file.h5'\n",
    "\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor()#,transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
    "    ])\n",
    "    #sr_data = srDataset( lr_dir = lr_path,hr_dir = hr_path, mode='train', transform=data_transform)\n",
    "    generator = Generator().to(device)\n",
    "    print('# generator parameters:', sum(param.numel() for param in generator.parameters()))\n",
    "    discriminator = Discriminator().to(device) #Discriminator\n",
    "    print('# discriminator parameters:', sum(param.numel() for param in discriminator.parameters()))\n",
    "    #C:\\Users\\maijieai\\pratice\\p1\\torch_files\\SRGAN-20220509T100244Z-002\\SRGAN\\save_models\n",
    "    generator.load_state_dict(torch.load('C:/Users/maijieai/pratice/p1/torch_files/SRGAN-c/SRGAN/save_models/9gmodel_weights2.pth'))\n",
    "    discriminator.load_state_dict(torch.load('save_models/9dmodel_weights2.pth'))\n",
    "    generator_criterion = GeneratorLoss().to(device)\n",
    "    #print(sr_data[5][0].shape)  # 此处为__getitem__的用法\n",
    "    \n",
    "    #train_model( generator,discriminator,sr_data,10,64 ,generator_criterion,device,learning_rate)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1651576429395,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "DocLdX4mm_Ii",
    "outputId": "b8820912-4f65-458f-8c47-7dc54df5e7af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(256, 256, 4)\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 464
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "error",
     "timestamp": 1652084886391,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "DvqpO2JB0qsX",
    "outputId": "720b5142-3136-4457-c97c-4dfad1abb699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(120, 125, 3)\n",
      "(480, 500, 3)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'memoryview' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-d5119de67a16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0ms1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_simaget\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_transform\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mcr\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mssi\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d5119de67a16>\u001b[0m in \u001b[0;36mpredict_simaget\u001b[1;34m(model, img_path, img_path2, transforms1, device, num)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#p2 = psnr ( target,imgn )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#sr_image = model(lr_image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpren\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mpsnr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mssim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpytorch_ssim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mssim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpren\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'memoryview' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "import pytorch_ssim\n",
    "\n",
    "def predict_simaget ( model,img_path,img_path2,transforms1,device,num):\n",
    "    img = Image.open(img_path)\n",
    "    target = Image.open(img_path2)\n",
    "    imgt = transforms1(img).unsqueeze(0)\n",
    "    imgt = imgt.to(device)\n",
    "    \n",
    "    imgn = np.array (img)\n",
    "    target = np.array ( target)\n",
    "    print ( imgn.shape)\n",
    "    \n",
    "    pre = model(imgt)\n",
    "    pren = pre.cpu().detach().numpy().squeeze(0)\n",
    "    pren *= 225\n",
    "    pren = np.float32(pren).transpose(1,2,0)\n",
    "    #pre_t = detransform(pre,device)\n",
    "    #print ( imgn)\n",
    "    #print ( pre_t)\n",
    "    pre_s = pren.astype(np.uint8)\n",
    "    pre_s1 = Image.fromarray ( pre_s)\n",
    "    #pre_s1.save (\"test_set5/normal_data/test_image\"+str(num)+\".png\")\n",
    "    print ( pren.shape)\n",
    "    p1 = psnr (target,pren)\n",
    "    p3 = PSNR ( pren,target)\n",
    "    #p4 = PSNR (imgn,target)\n",
    "    #p2 = psnr ( target,imgn )\n",
    "    #sr_image = model(lr_image)\n",
    "    mse = ((target - pren) ** 2).data.mean()\n",
    "    psnr2 = 10 * log10(1 / mse)\n",
    "    ssim = pytorch_ssim.ssim(pren, target).data[0]\n",
    "    print ( pnsr2)\n",
    "    print ( ssim )\n",
    "    print ( p1)\n",
    "    print ( p3)\n",
    "    #print ( p4)\n",
    "    #print ( p2)\n",
    "    return ssim\n",
    "\n",
    "cr = 1\n",
    "ssi = 0\n",
    "for i in range(9):\n",
    "    \n",
    "    s1 =0\n",
    "    \n",
    "    p1 = \"C:/e/SR_datasets/SR/test/set14/t\"+str(i+1)+\".png\"\n",
    "    p2 = \"C:/e/SR_datasets/SR/test/set14/down\"+str(i+1)+\".png\"\n",
    "    p3 = \"C:/e/SR_datasets/SR/test/set14/cubic\"+str(i+1)+\".png\"\n",
    "    print (i)\n",
    "    if (i==3)|(i==4)|(i==8) :\n",
    "        continue\n",
    "        \n",
    "    s1=predict_simaget ( generator,p2,p1,data_transform,device,i+1)\n",
    "    cr+=1\n",
    "    ssi+=s1\n",
    "    \n",
    "print ( ssi/(cr*1.0))\n",
    "\n",
    "img_path = 'test_imgs/b_4down1.jpg'\n",
    "img_path2 = 'test_imgs/baby.png'\n",
    "#predict_simaget ( generator,img_path,img_path2,data_transform,device,31)\n",
    "img_path = 'test_imgs/b_4down2.jpg'\n",
    "img_path2 = 'test_imgs/woman.png'\n",
    "#predict_simaget ( generator,img_path,img_path2,data_transform,device,41)\n",
    "'''\n",
    "sr_image = model(lr_image)\n",
    "mse = ((hr_image - sr_image) ** 2).data.mean()\n",
    "psnr = 10 * log10(1 / mse)\n",
    "ssim = pytorch_ssim.ssim(sr_image, hr_image).data[0]\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1652082419975,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "sQxGsfIgqltg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 36433,
     "status": "ok",
     "timestamp": 1652082760999,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "iMYv_ZJZ1s27",
    "outputId": "6a145cf2-a1b7-435d-fe13-5220fae11aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17695,
     "status": "ok",
     "timestamp": 1652082476181,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "6P5Xr87HAZdt",
    "outputId": "0cc15e08-f4cb-4365-e29b-a04fa3de0da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (7.1.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.21.6)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (3.2.2)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.4.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
      "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (0.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.11.0+cu113)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchtext) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.11.0+cu113)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision) (4.2.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.11.0+cu113)\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.11.0+cu113)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0->torchaudio) (4.2.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image\n",
    "!pip install glob2\n",
    "!pip install torch\n",
    "!pip install torchtext \n",
    "!pip install torchvision\n",
    "!pip install torchaudio\n",
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1651567256916,
     "user": {
      "displayName": "xiao Mai",
      "userId": "06780571542591735896"
     },
     "user_tz": -480
    },
    "id": "HirmMMJW1P_Q",
    "outputId": "0c104c19-4c8e-477d-d4c9-55efe253e23a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  3 08:40:54 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXoC9if3CXbj"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SRGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "328af6f71b4f436f84a27e7a4d5a11e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b12ba0ab2f44e719d5921920817279c",
       "IPY_MODEL_76426373bc3a4ffda6c3984a9f0f2a5f",
       "IPY_MODEL_bd0c4e632de44d0f8e6a62a91b9739b4"
      ],
      "layout": "IPY_MODEL_e17110f63fe94a03977d94b2052e81ce"
     }
    },
    "49a2b0898de448d69e2f91f9c93dd7ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76426373bc3a4ffda6c3984a9f0f2a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b4e66387fe44459ad16c28166c71215",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_865f163380284253b5eb8ab2b4de895d",
      "value": 553433881
     }
    },
    "7b12ba0ab2f44e719d5921920817279c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ccae21f1b2f49368bbafb3527775d1d",
      "placeholder": "​",
      "style": "IPY_MODEL_49a2b0898de448d69e2f91f9c93dd7ca",
      "value": "100%"
     }
    },
    "865f163380284253b5eb8ab2b4de895d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ccae21f1b2f49368bbafb3527775d1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b4e66387fe44459ad16c28166c71215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a25bdf0d56a44bf4823caf22e83e17b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2cdaee2a47c40bba7ce98e8f53a4014": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd0c4e632de44d0f8e6a62a91b9739b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2cdaee2a47c40bba7ce98e8f53a4014",
      "placeholder": "​",
      "style": "IPY_MODEL_a25bdf0d56a44bf4823caf22e83e17b5",
      "value": " 528M/528M [00:02&lt;00:00, 218MB/s]"
     }
    },
    "e17110f63fe94a03977d94b2052e81ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
